The objective is to do multi-modal analysis - analyze two dierent modes (text, images) of data about the
same object and use them to detect specific objects by extracting features, learning labels and building classifers (binary in our context).

A multi-modal data for cultural informatics - a group of painters of Indian origin, paint scrolls and write songs about them. 

The goal is to preserve these scrolls and their songs and help search on multi-modal datasets generated from them. For e.g. art connoisseurs may be interested in searching for scrolls which have mythical fgures in them and so on. The search engine
will then return only those scrolls that have this specifc feature.

Data set: 

Labels: Different Annotators labeled each image in the directory, according to whether a tree, mythical character or an animal was present or not.
Labels for each category (tree, mythical character and animal) is genearated using a majority vote of the labels
generated by every one. For e.g. if there are three annotators A1, A2 and A3 all labeling the tree
{ and their votes are 1, 0, 1 respectively, then majority vote says that the nal label is 1 (since
the number of 1's (two) is greater than 0's (one)). This simulates a real-life setting where labels
can be noisy, incorrect and biased. At the end of this phase, obtained majority
vote labels for each of the three tasks - identification of tree, mythical character and animal.

Features: Extracted two kinds of features - (a) image and (b) text, using the python script 

Preprocessing: Split the data set into train and test sets and then performed these three experiments
for each object to be identified (tree, mythical character and animal):
Experiment 1: Extract only image features.
Experiment 2: Extract only text features
Experiment 3: Extract both text and image features

Bult Linear Regression model in R on the training data in Experiment 1, 2 and 3 and used the Mean Square Error as the error metric of your models.